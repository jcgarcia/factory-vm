!<arch>
//                                              272       `
cache-manager.sh/
install-jenkins.sh/
configure-jenkins.sh/
install-awscli.sh/
install-caddy.sh/
install-certificates.sh/
install-docker.sh/
install-jcscripts.sh/
install-k8s-tools.sh/
install-terraform.sh/
expand-data-disk.sh/
install-android-sdk.sh/
install-ansible.sh/
/0              0           0     0     644     12598     `
#!/bin/bash
# cache-manager.sh - Download and cache tools for Factory VM
#
# Downloads and caches:
# - Tool binaries (Terraform, kubectl, Helm)
# - AWS CLI
# - Jenkins Docker image
# - Jenkins plugins
#
# All downloads are cached in tools/cache/ to speed up subsequent installations

################################################################################
# Configuration
################################################################################

# Get script directory (tools/lib/)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Use existing CACHE_DIR if set (from setup-factory-vm.sh), otherwise use local cache
# This allows ~/.factory-vm/cache when called from installer, or tools/cache when standalone
CACHE_DIR="${CACHE_DIR:-$(dirname "$SCRIPT_DIR")/cache}"

# Tool versions - will be detected at runtime
export TERRAFORM_VERSION="${TERRAFORM_VERSION:-}"
export KUBECTL_VERSION="${KUBECTL_VERSION:-}"
export HELM_VERSION="${HELM_VERSION:-}"
export JENKINS_VERSION="${JENKINS_VERSION:-lts-jdk21}"

################################################################################
# Version Detection
################################################################################

get_latest_alpine_version() {
    # Try to get latest stable version from Alpine's release page
    local latest_version=$(curl -s https://dl-cdn.alpinelinux.org/alpine/latest-stable/releases/${ALPINE_ARCH}/latest-releases.yaml 2>/dev/null | grep -m1 'version:' | awk '{print $2}' | cut -d. -f1,2)
    
    # Fallback to known stable version if auto-detection fails
    if [ -z "$latest_version" ]; then
        echo "3.22"
    else
        echo "$latest_version"
    fi
}

get_latest_terraform_version() {
    local latest=$(curl -sL https://api.github.com/repos/hashicorp/terraform/releases/latest 2>/dev/null | grep '"tag_name"' | sed -E 's/.*"v([^"]+)".*/\1/')
    echo "${latest:-1.6.6}"
}

get_latest_kubectl_version() {
    local latest=$(curl -sL https://dl.k8s.io/release/stable.txt 2>/dev/null | sed 's/^v//')
    echo "${latest:-1.28.4}"
}

get_latest_helm_version() {
    local latest=$(curl -sL https://api.github.com/repos/helm/helm/releases/latest 2>/dev/null | grep '"tag_name"' | sed -E 's/.*"v([^"]+)".*/\1/')
    echo "${latest:-3.13.3}"
}

################################################################################
# Download Functions
################################################################################

download_and_cache_terraform() {
    local version="$1"
    local cache_file="${CACHE_DIR}/terraform/terraform_${version}_linux_arm64.zip"
    
    if [ -f "$cache_file" ]; then
        log_info "Terraform ${version} already cached"
        return 0
    fi
    
    log_info "Downloading Terraform ${version}..."
    mkdir -p "${CACHE_DIR}/terraform"
    if curl -sL "https://releases.hashicorp.com/terraform/${version}/terraform_${version}_linux_arm64.zip" \
        -o "$cache_file"; then
        log_success "Terraform ${version} cached"
    else
        log_error "Failed to download Terraform"
        return 1
    fi
}

download_and_cache_kubectl() {
    local version="$1"
    local cache_file="${CACHE_DIR}/kubectl/kubectl_${version}"
    
    if [ -f "$cache_file" ]; then
        log_info "kubectl ${version} already cached"
        return 0
    fi
    
    log_info "Downloading kubectl ${version}..."
    mkdir -p "${CACHE_DIR}/kubectl"
    if curl -sL "https://dl.k8s.io/release/v${version}/bin/linux/arm64/kubectl" \
        -o "$cache_file"; then
        chmod +x "$cache_file"
        log_success "kubectl ${version} cached"
    else
        log_error "Failed to download kubectl"
        return 1
    fi
}

download_and_cache_helm() {
    local version="$1"
    local cache_file="${CACHE_DIR}/helm/helm-v${version}-linux-arm64.tar.gz"
    
    if [ -f "$cache_file" ]; then
        log_info "Helm ${version} already cached"
        return 0
    fi
    
    log_info "Downloading Helm ${version}..."
    mkdir -p "${CACHE_DIR}/helm"
    if curl -sL "https://get.helm.sh/helm-v${version}-linux-arm64.tar.gz" \
        -o "$cache_file"; then
        log_success "Helm ${version} cached"
    else
        log_error "Failed to download Helm"
        return 1
    fi
}

download_and_cache_awscli() {
    local cache_file="${CACHE_DIR}/awscli/awscli-latest-aarch64.zip"
    
    # Debug: Show what we're looking for
    [ "${DEBUG:-}" = "1" ] && echo "[DEBUG] Checking for AWS CLI cache at: $cache_file" >&2
    
    if [ -f "$cache_file" ]; then
        log_info "AWS CLI already cached"
        return 0
    fi
    
    log_info "Downloading AWS CLI v2..."
    mkdir -p "${CACHE_DIR}/awscli"
    
    # Use temporary file to avoid partial downloads
    local temp_file="${cache_file}.tmp"
    if curl -sL "https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip" \
        -o "$temp_file" && [ -s "$temp_file" ]; then
        mv "$temp_file" "$cache_file"
        log_success "AWS CLI cached"
        return 0
    else
        rm -f "$temp_file"
        log_error "Failed to download AWS CLI"
        return 1
    fi
}

download_and_cache_ansible() {
    local cache_file="${CACHE_DIR}/ansible/ansible-requirements.txt"
    
    if [ -f "$cache_file" ]; then
        log_info "Ansible requirements already cached"
        return 0
    fi
    
    log_info "Creating Ansible requirements file..."
    mkdir -p "${CACHE_DIR}/ansible"
    
    # Create a requirements file for pip to cache
    cat > "$cache_file" << 'EOF'
ansible>=2.16
boto3>=1.34
botocore>=1.34
EOF
    
    log_success "Ansible requirements cached"
    return 0
}

download_and_cache_jenkins_image() {
    local cache_file="${CACHE_DIR}/jenkins/jenkins-lts-jdk21.tar"
    
    if [ -f "$cache_file" ]; then
        log_info "Jenkins Docker image already cached"
        return 0
    fi
    
    log_info "Downloading Jenkins Docker image (jenkins/jenkins:lts-jdk21)..."
    log_info "This is a ~1.5GB download, will take 8-10 minutes on first run"
    mkdir -p "${CACHE_DIR}/jenkins"
    
    # Use skopeo to download Docker image without requiring Docker daemon
    if command -v skopeo &>/dev/null; then
        # Download using skopeo with OCI format (preserves tags correctly)
        if skopeo copy docker://jenkins/jenkins:lts-jdk21 oci-archive:"$cache_file":lts-jdk21 2>/dev/null; then
            log_success "Jenkins image cached ($(du -h "$cache_file" | cut -f1))"
            return 0
        else
            rm -f "$cache_file"
            log_error "Failed to download Jenkins image with skopeo"
            return 1
        fi
    else
        log_warning "skopeo not found, Jenkins image will be downloaded in VM"
        return 1
    fi
}

################################################################################
# Batch Cache Functions
################################################################################

cache_all_tools() {
    log "Downloading and caching installation files..."
    log_info "First-time downloads will be cached for faster subsequent installations"
    echo ""
    
    # Detect versions
    TERRAFORM_VERSION=$(get_latest_terraform_version)
    KUBECTL_VERSION=$(get_latest_kubectl_version)
    HELM_VERSION=$(get_latest_helm_version)
    
    log_info "Tool versions detected:"
    log_info "  Terraform: ${TERRAFORM_VERSION}"
    log_info "  kubectl: ${KUBECTL_VERSION}"
    log_info "  Helm: ${HELM_VERSION}"
    echo ""
    
    # Download in parallel (background jobs)
    download_and_cache_terraform "$TERRAFORM_VERSION" &
    local terraform_pid=$!
    download_and_cache_kubectl "$KUBECTL_VERSION" &
    local kubectl_pid=$!
    download_and_cache_helm "$HELM_VERSION" &
    local helm_pid=$!
    download_and_cache_awscli &
    local awscli_pid=$!
    download_and_cache_ansible &
    local ansible_pid=$!
    download_and_cache_jenkins_image &
    local jenkins_pid=$!
    
    # Wait for parallel downloads
    wait $terraform_pid
    wait $kubectl_pid
    wait $helm_pid
    wait $awscli_pid
    wait $ansible_pid
    wait $jenkins_pid
    
    log_success "All tools cached and ready for installation"
    echo ""
}

################################################################################
# Jenkins Plugin Caching
################################################################################

download_and_cache_plugin() {
    local plugin_name="$1"
    local cache_file="${CACHE_DIR}/jenkins/plugins/${plugin_name}.hpi"
    
    if [ -f "$cache_file" ]; then
        return 0  # Already cached, silent success
    fi
    
    mkdir -p "${CACHE_DIR}/jenkins/plugins"
    
    # Download plugin using Jenkins update center
    local download_url="https://updates.jenkins.io/latest/${plugin_name}.hpi"
    
    if curl -sL "$download_url" -o "$cache_file" 2>/dev/null; then
        return 0
    else
        rm -f "$cache_file"  # Clean up partial download
        return 1
    fi
}

cache_all_plugins() {
    log "Downloading and caching Jenkins plugins..."
    log_info "First-time downloads will be cached for faster subsequent installations"
    echo ""
    
    # Ensure we don't exit on error during plugin downloads
    set +e
    
    # Plugin list (from plugins.txt in heredoc)
    local plugins=(
        "configuration-as-code"
        "git"
        "git-client"
        "github"
        "github-branch-source"
        "docker-plugin"
        "docker-workflow"
        "workflow-aggregator"
        "pipeline-stage-view"
        "pipeline-github-lib"
        "blueocean"
        "credentials"
        "credentials-binding"
        "plain-credentials"
        "ssh-credentials"
        "aws-credentials"
        "aws-java-sdk"
        "gradle"
        "nodejs"
        "kubernetes"
        "kubernetes-cli"
        "timestamper"
        "build-timeout"
        "ws-cleanup"
        "ansicolor"
    )
    
    local total=${#plugins[@]}
    local cached=0
    local downloaded=0
    local failed=0
    
    log_info "Checking cache for ${total} plugins..."
    echo ""
    
    # Download plugins in parallel (5 at a time to avoid overwhelming the server)
    local batch_size=5
    local i=0
    
    while [ $i -lt $total ]; do
        local pids=()
        local batch_plugins=()
        
        # Start batch of downloads
        for j in $(seq 0 $((batch_size - 1))); do
            local idx=$((i + j))
            if [ $idx -ge $total ]; then
                break
            fi
            
            local plugin="${plugins[$idx]}"
            batch_plugins+=("$plugin")
            
            # Check if already cached
            if [ -f "${CACHE_DIR}/jenkins/plugins/${plugin}.hpi" ]; then
                ((cached++))
                echo "  [${idx}/${total}] ${plugin} (cached)"
            else
                echo -n "  [${idx}/${total}] ${plugin} (downloading)..."
                download_and_cache_plugin "$plugin" &
                pids+=($!)
            fi
        done
        
        # Wait for batch to complete
        for pid in "${pids[@]}"; do
            wait $pid && local result=0 || local result=$?
            if [ $result -eq 0 ]; then
                ((downloaded++))
                echo " ✓"
            else
                ((failed++))
                echo " ✗"
            fi
        done
        
        i=$((i + batch_size))
    done
    
    echo ""
    log_info "Plugin cache summary:"
    log_info "  Already cached: ${cached}"
    log_info "  Downloaded: ${downloaded}"
    if [ $failed -gt 0 ]; then
        log_warning "  Failed: ${failed} (will download inside VM)"
    fi
    
    log_success "Plugins cached and ready for installation"
    echo ""
    
    # Re-enable exit on error
    set -e
}

################################################################################
# Module Initialization
################################################################################

# Verify this module is being sourced, not executed
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    echo "ERROR: cache-manager.sh should be sourced, not executed directly"
    echo "Usage: source ${BASH_SOURCE[0]}"
    exit 1
fi

# Export all functions
export -f get_latest_alpine_version
export -f get_latest_terraform_version
export -f get_latest_kubectl_version
export -f get_latest_helm_version
export -f download_and_cache_terraform
export -f download_and_cache_kubectl
export -f download_and_cache_helm
export -f download_and_cache_awscli
export -f download_and_cache_ansible
export -f download_and_cache_jenkins_image
export -f cache_all_tools
export -f download_and_cache_plugin
export -f cache_all_plugins
/18             0           0     0     644     10966     `
#!/bin/bash
# install-jenkins.sh - Install Jenkins on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_jenkins_via_ssh() {
    log_info "Installing Jenkins via SSH..."
    
    # Check if Jenkins image is cached on data disk
    local jenkins_cached=0
    if ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "test -f /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar" 2>/dev/null; then
        jenkins_cached=1
        log_info "  ✓ Jenkins image already cached on data disk (skipping 475MB copy - saves ~1 minute!)"
    elif [ -f "${CACHE_DIR}/jenkins/jenkins-lts-jdk21.tar" ]; then
        log_info "  Copying Jenkins Docker image from host cache (475MB, this may take a minute)..."
        scp -i "$VM_SSH_PRIVATE_KEY" -P "$VM_SSH_PORT" \
            -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "${CACHE_DIR}/jenkins/jenkins-lts-jdk21.tar" \
            foreman@localhost:/var/cache/factory-build/jenkins/
        jenkins_cached=1
    fi
    
    # Install Jenkins - Phase 2 approach: create config BEFORE starting container
    # Pass password to VM via environment variable (can't use heredoc variable expansion
    # because we need the quoted heredoc 'EOF' to protect other special characters)
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost "export JENKINS_FOREMAN_PASSWORD='${JENKINS_FOREMAN_PASSWORD}'; bash -s" << 'EOF'
# Load Jenkins image
if [ -f "/var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar" ]; then
    echo "Loading Jenkins Docker image from cache..."
    # Load OCI archive format with skopeo (preserves tags)
    if command -v skopeo &>/dev/null; then
        sudo skopeo copy oci-archive:/var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar:lts-jdk21 docker-daemon:jenkins/jenkins:lts-jdk21
    else
        # Fallback to docker load (may not preserve tag)
        sudo docker load -i /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
    fi
    
    # Verify the image was loaded correctly with the right tag
    if ! sudo docker images jenkins/jenkins:lts-jdk21 | grep -q lts-jdk21; then
        echo "⚠ Cached image doesn't have correct tag, re-downloading..."
        sudo docker pull jenkins/jenkins:lts-jdk21
        
        # Update cache with new OCI format
        echo "Updating cache with new image (OCI format)..."
        sudo rm -f /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
        if command -v skopeo &>/dev/null; then
            sudo skopeo copy docker-daemon:jenkins/jenkins:lts-jdk21 oci-archive:/var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar:lts-jdk21
        else
            sudo docker save jenkins/jenkins:lts-jdk21 -o /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
        fi
        sudo chown foreman:foreman /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
    fi
else
    echo "Pulling Jenkins Docker image from Docker Hub (this may take several minutes)..."
    sudo docker pull jenkins/jenkins:lts-jdk21
    
    # Save to cache for next time (OCI format preserves tags)
    echo "Saving Jenkins image to cache (OCI format)..."
    sudo mkdir -p /var/cache/factory-build/jenkins
    if command -v skopeo &>/dev/null; then
        sudo skopeo copy docker-daemon:jenkins/jenkins:lts-jdk21 oci-archive:/var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar:lts-jdk21
    else
        sudo docker save jenkins/jenkins:lts-jdk21 -o /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
    fi
    sudo chown foreman:foreman /var/cache/factory-build/jenkins/jenkins-lts-jdk21.tar
fi

# Create Jenkins home directory structure
echo "Creating Jenkins configuration directory..."
sudo mkdir -p /opt/jenkins/init.groovy.d
sudo chown -R 1000:1000 /opt/jenkins

# Create init script to skip setup wizard
echo "Creating init scripts..."
sudo tee /opt/jenkins/init.groovy.d/01-skip-wizard.groovy > /dev/null << 'GROOVY_SKIP'
#!groovy
import jenkins.model.Jenkins
import jenkins.install.InstallState

def instance = Jenkins.getInstance()
if (!instance.installState.isSetupComplete()) {
    instance.setInstallState(InstallState.INITIAL_SETUP_COMPLETED)
    instance.save()
    println '--> Setup wizard skipped'
}
GROOVY_SKIP

# Create init script to configure Jenkins URL and built-in node
sudo tee /opt/jenkins/init.groovy.d/02-configure-jenkins.groovy > /dev/null << 'GROOVY_CONFIG'
#!groovy
import jenkins.model.Jenkins
import jenkins.model.JenkinsLocationConfiguration

def instance = Jenkins.getInstance()

// Set Jenkins URL
def locationConfig = JenkinsLocationConfiguration.get()
locationConfig.setUrl('https://factory.local/')
locationConfig.save()
println '--> Jenkins URL set to https://factory.local/'

// Disable builds on built-in node (security best practice)
instance.setNumExecutors(0)
instance.setMode(hudson.model.Node.Mode.EXCLUSIVE)
instance.save()
println '--> Built-in node configured (executors disabled - use agents for builds)'
GROOVY_CONFIG

# Create foreman user with API token
sudo tee /opt/jenkins/init.groovy.d/05-create-foreman-user.groovy > /dev/null << 'GROOVY_USER'
#!groovy
import jenkins.model.Jenkins
import hudson.model.User
import hudson.security.HudsonPrivateSecurityRealm
import hudson.security.FullControlOnceLoggedInAuthorizationStrategy
import jenkins.security.ApiTokenProperty

def instance = Jenkins.getInstance()

println '--> Creating foreman user with API token...'

// Get foreman password from environment
def foremanPassword = System.getenv('JENKINS_FOREMAN_PASSWORD')
if (!foremanPassword) {
    foremanPassword = 'changeme123'
    println 'WARNING: Using default password'
}

// Create security realm with foreman user
def hudsonRealm = new HudsonPrivateSecurityRealm(false)
hudsonRealm.createAccount('foreman', foremanPassword)
instance.setSecurityRealm(hudsonRealm)

// Set authorization strategy
def strategy = new FullControlOnceLoggedInAuthorizationStrategy()
strategy.setAllowAnonymousRead(false)
instance.setAuthorizationStrategy(strategy)

instance.save()

// Get the created user and generate API token
def user = User.get('foreman', false)
def tokenStore = user.getProperty(ApiTokenProperty.class)
if (tokenStore == null) {
    tokenStore = new ApiTokenProperty()
    user.addProperty(tokenStore)
}

// Create token
def result = tokenStore.tokenStore.generateNewToken("CLI Access")
def tokenValue = result.plainValue

// Save token to file
new File('/var/jenkins_home/foreman-api-token.txt').text = tokenValue

println "--> Foreman user created successfully"
println "    API Token saved to: /var/jenkins_home/foreman-api-token.txt"

user.save()
instance.save()
GROOVY_USER

# Note: Docker Cloud agent is configured via JCasC (jenkins.yaml) below
# Groovy init scripts run BEFORE plugins load, so we can't reference plugin classes here
# JCasC is applied AFTER plugins load - this is the correct approach

# Create JCasC configuration for Docker Cloud agent
sudo tee /opt/jenkins/jenkins.yaml > /dev/null << 'CASC_CONFIG'
jenkins:
  clouds:
    - docker:
        name: "docker"
        dockerApi:
          dockerHost:
            uri: "unix:///var/run/docker.sock"
        templates:
          - labelString: "docker-agent linux arm64"
            dockerTemplateBase:
              image: "jenkins/inbound-agent:latest"
            remoteFs: "/home/jenkins/agent"
            instanceCapStr: "10"
            connector:
              attach:
                user: "jenkins"
CASC_CONFIG

echo "  ✓ JCasC configuration created for Docker Cloud agent"

# Set ownership
sudo chown -R 1000:1000 /opt/jenkins

# Install plugins BEFORE starting Jenkins using jenkins-plugin-cli
# This is crucial: plugins must be installed BEFORE init scripts run
# because the Docker Cloud groovy script imports plugin classes at compile time
echo "Installing Jenkins plugins (this ensures Docker Cloud agent works)..."

# Create plugins.txt with required plugins
# jenkins-plugin-cli will automatically resolve all dependencies
cat > /tmp/plugins.txt << 'PLUGINS_LIST'
# Core plugins needed for Docker Cloud agent
docker-plugin
docker-workflow

# Git and GitHub integration
git
git-client
github
github-branch-source

# Pipeline support
workflow-aggregator
pipeline-stage-view
pipeline-github-lib

# Configuration as Code
configuration-as-code

# Blue Ocean UI
blueocean

# Credentials management
credentials
credentials-binding
plain-credentials
ssh-credentials

# Cloud providers
aws-credentials
kubernetes
kubernetes-cli

# Build tools
gradle
nodejs

# Utility plugins
timestamper
build-timeout
ws-cleanup
ansicolor
PLUGINS_LIST

# Run jenkins-plugin-cli in a temporary container to install plugins
# This downloads plugins WITH all their dependencies to /opt/jenkins/plugins
echo "  Running jenkins-plugin-cli to install plugins and dependencies..."
sudo docker run --rm \
    -v /opt/jenkins:/var/jenkins_home \
    -v /tmp/plugins.txt:/plugins.txt \
    jenkins/jenkins:lts-jdk21 \
    jenkins-plugin-cli --plugin-file /plugins.txt --plugin-download-directory /var/jenkins_home/plugins

if [ $? -eq 0 ]; then
    echo "  ✓ Plugins installed successfully"
    # Fix ownership after plugin installation
    sudo chown -R 1000:1000 /opt/jenkins/plugins
else
    echo "  ⚠ Plugin installation had errors (Jenkins may still work)"
fi

# Start Jenkins container with volume mounts
# - /opt/jenkins: Jenkins config, init scripts, plugins (system disk)
# - /var/lib/jenkins: Job workspaces (data disk - preserved across reinstalls)
echo "Starting Jenkins container..."

# Remove any existing jenkins container (may exist from restored cache disk)
if sudo docker ps -a --format '{{.Names}}' | grep -q '^jenkins$'; then
    echo "  Removing existing jenkins container..."
    sudo docker stop jenkins 2>/dev/null || true
    sudo docker rm jenkins 2>/dev/null || true
fi

sudo docker run -d --name jenkins \
    --restart always \
    -p 8080:8080 -p 50000:50000 \
    -v /opt/jenkins:/var/jenkins_home \
    -v /var/lib/jenkins:/var/jenkins_home/workspace \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -e JAVA_OPTS="-Djenkins.install.runSetupWizard=false -Xmx2g" \
    -e JENKINS_FOREMAN_PASSWORD="${JENKINS_FOREMAN_PASSWORD}" \
    -e CASC_JENKINS_CONFIG="/var/jenkins_home/jenkins.yaml" \
    jenkins/jenkins:lts-jdk21

echo "✓ Jenkins container started"
echo "  Waiting for Jenkins to initialize and process init scripts..."
sleep 5
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Jenkins installed and starting"
    else
        log_error "Jenkins installation failed"
        return 1
    fi
}

# Export functions
export -f install_jenkins_via_ssh
common.sh/      0           0     0     644     5464      `
#!/bin/bash
# common.sh - Shared utilities for Factory VM setup
# 
# Provides:
# - Logging functions (log, log_error, log_warning, log_info, log_success)
# - Color codes for terminal output
# - SSH execution wrapper (ssh_exec)
# - Password generation
# - Configuration variables

################################################################################
# Color Codes
################################################################################

# ANSI color codes for terminal output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m'  # No Color / Reset

################################################################################
# Configuration Variables
################################################################################

# VM Basic Configuration
export VM_DIR="${VM_DIR:-${HOME}/vms/factory}"
export VM_NAME="${VM_NAME:-factory}"
export VM_MEMORY="${VM_MEMORY:-4G}"
export VM_CPUS="${VM_CPUS:-4}"
export VM_SSH_PORT="${VM_SSH_PORT:-2222}"

# VM Network and Identity
export VM_HOSTNAME="${VM_HOSTNAME:-factory.local}"
export VM_USERNAME="${VM_USERNAME:-foreman}"
export SSH_KEY_NAME="${SSH_KEY_NAME:-factory-foreman}"

# Disk Configuration
export SYSTEM_DISK_SIZE="${SYSTEM_DISK_SIZE:-50G}"
export DATA_DISK_SIZE="${DATA_DISK_SIZE:-50G}"
export SYSTEM_DISK="${SYSTEM_DISK:-${VM_DIR}/${VM_NAME}.qcow2}"
export DATA_DISK="${DATA_DISK:-${VM_DIR}/${VM_NAME}-data.qcow2}"

# SSH Configuration
export VM_SSH_PRIVATE_KEY="${VM_SSH_PRIVATE_KEY:-${HOME}/.ssh/${SSH_KEY_NAME}}"
export VM_SSH_PUBLIC_KEY="${VM_SSH_PUBLIC_KEY:-${HOME}/.ssh/${SSH_KEY_NAME}.pub}"

# Security - Passwords generated at runtime
export JENKINS_FOREMAN_PASSWORD="${JENKINS_FOREMAN_PASSWORD:-}"

# Alpine Configuration
export ALPINE_VERSION="${ALPINE_VERSION:-3.22}"
export ALPINE_ARCH="${ALPINE_ARCH:-aarch64}"

################################################################################
# Logging Functions
################################################################################

log() {
    echo -e "${GREEN}[$(date +'%H:%M:%S')]${NC} $*"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $*" >&2
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $*"
}

log_info() {
    echo -e "${BLUE}[INFO]${NC} $*"
}

log_success() {
    echo -e "${GREEN}[✓]${NC} $*"
}

################################################################################
# Utility Functions
################################################################################

# Generate cryptographically secure random password
generate_secure_password() {
    # Generate 20-character password with letters, numbers, and safe symbols
    openssl rand -base64 32 | tr -d "=+/" | cut -c1-20
}

# Execute command via SSH with standard options
# Usage: ssh_exec "command to run"
ssh_exec() {
    ssh -i "$VM_SSH_PRIVATE_KEY" \
        -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no \
        -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 \
        -o ServerAliveInterval=30 \
        "${VM_USERNAME}@localhost" "$@"
}

# Check if a command exists
command_exists() {
    command -v "$1" &> /dev/null
}

# Wait with timeout
wait_with_timeout() {
    local timeout=$1
    local check_command=$2
    local elapsed=0
    
    while [ $elapsed -lt $timeout ]; do
        if eval "$check_command"; then
            return 0
        fi
        sleep 1
        ((elapsed++))
    done
    
    return 1
}

################################################################################
# Validation Functions
################################################################################

# Validate VM configuration
validate_config() {
    local errors=0
    
    if [[ ! "$VM_MEMORY" =~ ^[0-9]+[GMK]$ ]]; then
        log_error "Invalid VM_MEMORY format: $VM_MEMORY (should be like 4G, 2048M)"
        ((errors++))
    fi
    
    if [[ ! "$VM_CPUS" =~ ^[0-9]+$ ]] || [ "$VM_CPUS" -lt 1 ]; then
        log_error "Invalid VM_CPUS: $VM_CPUS (should be a positive integer)"
        ((errors++))
    fi
    
    if [[ ! "$VM_SSH_PORT" =~ ^[0-9]+$ ]] || [ "$VM_SSH_PORT" -lt 1024 ] || [ "$VM_SSH_PORT" -gt 65535 ]; then
        log_error "Invalid VM_SSH_PORT: $VM_SSH_PORT (should be 1024-65535)"
        ((errors++))
    fi
    
    return $errors
}

# Show current configuration
show_config() {
    log_info "Factory VM Configuration:"
    echo "  VM Name:          $VM_NAME"
    echo "  VM Hostname:      $VM_HOSTNAME"
    echo "  VM Username:      $VM_USERNAME"
    echo "  VM Memory:        $VM_MEMORY"
    echo "  VM CPUs:          $VM_CPUS"
    echo "  VM SSH Port:      $VM_SSH_PORT"
    echo "  System Disk Size: $SYSTEM_DISK_SIZE"
    echo "  Data Disk Size:   $DATA_DISK_SIZE"
    echo "  VM Directory:     $VM_DIR"
}

################################################################################
# Module Initialization
################################################################################

# Verify this module is being sourced, not executed
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    echo "ERROR: common.sh should be sourced, not executed directly"
    echo "Usage: source ${BASH_SOURCE[0]}"
    exit 1
fi

# Export all functions for use by other modules
export -f log log_error log_warning log_info log_success
export -f generate_secure_password
export -f ssh_exec
export -f command_exists wait_with_timeout
export -f validate_config show_config
/38             0           0     0     644     4495      `
#!/bin/bash
# configure-jenkins.sh - Configure Jenkins after installation
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

configure_jenkins_via_ssh() {
    log_info "Configuring Jenkins via SSH..."
    
    # Wait for Jenkins to be ready AND for init scripts to complete
    log_info "  Waiting for Jenkins to be fully initialized (this can take 2-3 minutes)..."
    log_info "  Init scripts should auto-create foreman user and API token..."
    
    local jenkins_ready=0
    local token_ready=0
    
    # First wait for Jenkins to respond
    for i in {1..90}; do
        if ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -p "$VM_SSH_PORT" foreman@localhost "curl -s http://localhost:8080/login >/dev/null 2>&1"; then
            jenkins_ready=1
            break
        fi
        if [ $((i % 10)) -eq 0 ]; then
            log_info "  Still waiting for Jenkins... ($((i * 2)) seconds elapsed)"
        fi
        sleep 2
    done
    
    if [ $jenkins_ready -eq 0 ]; then
        log_error "Jenkins did not become ready in time"
        return 1
    fi
    
    log_info "  Jenkins is responding, waiting for init scripts to complete..."
    
    # Now wait for the API token file to exist (init scripts completed)
    for i in {1..60}; do
        if ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -p "$VM_SSH_PORT" foreman@localhost "sudo docker exec jenkins test -f /var/jenkins_home/foreman-api-token.txt" 2>/dev/null; then
            token_ready=1
            break
        fi
        if [ $((i % 10)) -eq 0 ]; then
            log_info "  Waiting for API token... ($((i * 2)) seconds elapsed)"
        fi
        sleep 2
    done
    
    if [ $token_ready -eq 0 ]; then
        log_error "API token was not created - init scripts may have failed"
        return 1
    fi
    
    log_success "  ✓ Jenkins is ready"
    
    # Verify init scripts executed successfully and setup CLI
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
echo "Verifying Jenkins configuration..."

# Token already verified to exist by the installer
echo "✓ API token exists"

# Get Jenkins version for verification
JENKINS_VERSION=$(sudo docker exec jenkins java -jar /usr/share/jenkins/jenkins.war --version 2>/dev/null)
echo "✓ Jenkins version: $JENKINS_VERSION"

# Download Jenkins CLI jar
echo "Setting up Jenkins CLI..."
sudo mkdir -p /usr/local/share/jenkins
sudo docker cp jenkins:/usr/share/jenkins/jenkins-cli.jar /usr/local/share/jenkins/jenkins-cli.jar
sudo chmod 644 /usr/local/share/jenkins/jenkins-cli.jar
echo "✓ Jenkins CLI jar installed to /usr/local/share/jenkins/"

# Create jenkins-factory CLI function for VM users
sudo tee /etc/profile.d/jenkins-cli.sh > /dev/null << 'JENKINS_CLI_PROFILE'
# Jenkins CLI helper function
jenkins-factory() {
    local api_token
    
    # Get token from Jenkins container (use sudo if needed)
    if docker ps >/dev/null 2>&1; then
        api_token=$(docker exec jenkins cat /var/jenkins_home/foreman-api-token.txt 2>/dev/null | tr -d '\n\r')
    else
        api_token=$(sudo docker exec jenkins cat /var/jenkins_home/foreman-api-token.txt 2>/dev/null | tr -d '\n\r')
    fi
    
    if [ -z "$api_token" ]; then
        echo "Error: Could not retrieve API token from Jenkins" >&2
        return 1
    fi
    
    # Call Jenkins CLI with token directly in auth parameter
    # Use localhost:8080 (direct to container) instead of https://factory.local
    # This avoids SSL certificate issues with Caddy's self-signed cert
    java -jar /usr/local/share/jenkins/jenkins-cli.jar \
        -s http://localhost:8080 \
        -auth foreman:"$api_token" \
        -webSocket \
        "$@"
}
JENKINS_CLI_PROFILE
sudo chmod +x /etc/profile.d/jenkins-cli.sh
echo "✓ Jenkins CLI configured for VM users (jenkins-factory command)"

echo "✓ Jenkins configured successfully"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Jenkins configured successfully"
    else
        log_error "Jenkins configuration verification failed"
        return 1
    fi
}

# Export functions
export -f configure_jenkins_via_ssh

/60             0           0     0     644     1022      `
#!/bin/bash
# install-awscli.sh - Install AWS CLI on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_aws_cli_via_ssh() {
    log_info "Installing AWS CLI via SSH..."
    
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
echo "Installing AWS CLI (Alpine native package)..."
# Alpine Linux uses musl libc, official AWS CLI is built for glibc
# Use Alpine's native package which is compiled for musl
sudo apk add aws-cli

aws --version

echo "✓ AWS CLI installed"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ AWS CLI installed"
    else
        log_warning "AWS CLI installation had issues (continuing anyway)"
    fi
}

# Export functions
export -f install_aws_cli_via_ssh
install-base.sh/0           0     0     644     2823      `
#!/bin/bash
# install-base.sh - Install base packages on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_base_packages_via_ssh() {
    log_info "Installing base packages via SSH..."
    
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
# Configure APK cache on data disk for faster reinstalls
echo "Configuring APK cache..."
sudo mkdir -p /var/cache/factory-build/apk
sudo mkdir -p /var/cache/factory-build/apk-index
sudo mkdir -p /etc/apk/cache
sudo ln -sf /var/cache/factory-build/apk /etc/apk/cache

# Update package index and enable community repo
echo "Updating package index..."
sudo sed -i "/^#.*community/s/^#//" /etc/apk/repositories || \
    echo "http://dl-cdn.alpinelinux.org/alpine/v3.19/community" | sudo tee -a /etc/apk/repositories

# Check if we have cached APK index files (less than 24 hours old)
INDEX_CACHE_DIR="/var/cache/factory-build/apk-index"
INDEX_AGE_FILE="$INDEX_CACHE_DIR/.timestamp"
CACHE_VALID=0

if [ -f "$INDEX_AGE_FILE" ]; then
    CACHE_AGE=$(($(date +%s) - $(cat "$INDEX_AGE_FILE")))
    # If cache is less than 24 hours old (86400 seconds)
    if [ $CACHE_AGE -lt 86400 ]; then
        echo "Using cached APK index ($(($CACHE_AGE / 3600)) hours old)"
        CACHE_VALID=1
        # Copy cached indexes to APK's expected location
        sudo mkdir -p /var/cache/apk
        sudo cp -f "$INDEX_CACHE_DIR"/*.tar.gz /var/cache/apk/ 2>/dev/null || true
    else
        echo "APK index cache is stale ($(($CACHE_AGE / 3600)) hours old), refreshing..."
    fi
else
    echo "No cached APK index found, downloading..."
fi

# Update if cache is invalid or missing
if [ $CACHE_VALID -eq 0 ]; then
    sudo apk update
    # Save indexes to cache for next time
    sudo mkdir -p "$INDEX_CACHE_DIR"
    sudo cp -f /var/cache/apk/*.tar.gz "$INDEX_CACHE_DIR/" 2>/dev/null || true
    date +%s | sudo tee "$INDEX_AGE_FILE" > /dev/null
    echo "APK index cached for future use"
fi

# Install base packages (using cache if available)
echo "Installing base packages..."
sudo apk add \
    bash curl wget git vim nano openssh-client sudo \
    ca-certificates openssl gnupg tar gzip unzip bzip2 xz \
    coreutils findutils grep sed make gcc g++ musl-dev \
    python3 py3-pip nodejs npm openjdk21

echo "✓ Base packages installed"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Base packages installed"
    else
        log_error "Base package installation failed"
        return 1
    fi
}

# Export functions
export -f install_base_packages_via_ssh

/79             0           0     0     644     1954      `
#!/bin/bash
# install-caddy.sh - Install Caddy reverse proxy on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_caddy_via_ssh() {
    log_info "Installing Caddy reverse proxy via SSH..."
    
    # Create Caddyfile content as variable
    cat > /tmp/Caddyfile << 'CADDY_EOF'
factory.local {
    reverse_proxy localhost:8080
    tls internal
}
CADDY_EOF
    
    # Install Caddy
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
echo "Installing Caddy..."
sudo apk add caddy

echo "Creating Caddy configuration directory..."
sudo mkdir -p /etc/caddy
EOF
    
    # Copy Caddyfile
    scp -i "$VM_SSH_PRIVATE_KEY" -P "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        /tmp/Caddyfile foreman@localhost:/tmp/
    
    # Configure and start Caddy
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
sudo mv /tmp/Caddyfile /etc/caddy/Caddyfile
sudo chown root:root /etc/caddy/Caddyfile
sudo chmod 644 /etc/caddy/Caddyfile

echo "Starting Caddy service..."
sudo rc-update add caddy boot
sudo service caddy start

sleep 2

if sudo service caddy status | grep -q "started"; then
    echo "✓ Caddy is running"
else
    echo "⚠ Caddy may not be running properly"
fi
EOF
    
    rm -f /tmp/Caddyfile
    
    if [ $? -eq 0 ]; then
        log_success "✓ Caddy installed and configured"
    else
        log_error "Caddy installation failed"
        return 1
    fi
}

# Export functions
export -f install_caddy_via_ssh
/97             0           0     0     644     8260      `
#!/bin/bash
# install-certificates.sh - Install Caddy CA certificates on host system
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_certificates_on_host() {
    log "Installing Caddy CA certificates on host system..."
    
    local root_cert_file="${VM_DIR}/caddy-root-ca.crt"
    local intermediate_cert_file="${VM_DIR}/caddy-intermediate-ca.crt"
    local cert_installed=false
    
    # Wait a moment for Caddy to generate certificates
    sleep 2
    
    # Retrieve ROOT certificate from VM
    log_info "  Retrieving certificates from VM..."
    if ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=10 \
        foreman@localhost \
        "sudo cat /var/lib/caddy/.local/share/caddy/pki/authorities/local/root.crt" > "$root_cert_file" 2>/dev/null; then
        
        log_success "  ✓ Root certificate retrieved from VM"
        
        # Retrieve INTERMEDIATE certificate from VM (needed for full chain)
        if ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
            -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -o ConnectTimeout=10 \
            foreman@localhost \
            "sudo cat /var/lib/caddy/.local/share/caddy/pki/authorities/local/intermediate.crt" > "$intermediate_cert_file" 2>/dev/null; then
            log_success "  ✓ Intermediate certificate retrieved from VM"
        fi
        
        # Install to system trust store (REQUIRED for curl/wget to trust HTTPS)
        # CRITICAL: Remove ALL old Caddy certificates first (certs change on every VM start)
        log_info "    Removing old Caddy certificates from system trust store..."
        sudo rm -f /usr/local/share/ca-certificates/caddy*.crt 2>/dev/null || true
        sudo update-ca-certificates --fresh >/dev/null 2>&1 || true
        
        if sudo cp "$root_cert_file" /usr/local/share/ca-certificates/caddy-factory-ca.crt 2>/dev/null; then
            # Also install intermediate
            sudo cp "$intermediate_cert_file" /usr/local/share/ca-certificates/caddy-intermediate-ca.crt 2>/dev/null || true
            
            log_info "    Installing to system trust store..."
            sudo update-ca-certificates >/dev/null 2>&1
            
            if [ -f /usr/local/share/ca-certificates/caddy-factory-ca.crt ]; then
                log_success "  ✓ Certificates installed to system trust store"
                log_info "    https://factory.local is now trusted (no security warnings)"
                cert_installed=true
            fi
        else
            log_warning "  Could not install certificate to system trust store (sudo required)"
        fi
        
        # Install to browser certificate databases (Chrome/Chromium/Firefox)
        log_info "  Installing to browser certificate databases..."
        local browsers_updated=0
        
        # Install certutil if not present (needed for Chrome/Chromium/Firefox)
        if ! command -v certutil >/dev/null 2>&1; then
            log_info "  Installing libnss3-tools for browser certificate management..."
            sudo apt-get update -qq >/dev/null 2>&1 || true
            sudo apt-get install -y libnss3-tools >/dev/null 2>&1 || true
        fi
        
        if command -v certutil >/dev/null 2>&1; then
            # Helper function to completely remove all Caddy certificates (handles duplicates)
            remove_all_caddy_certs() {
                local db_path="$1"
                # Run deletion multiple times to catch duplicates (max 5 attempts)
                for i in {1..5}; do
                    certutil -D -d "$db_path" -n "Caddy Local CA - Factory" >/dev/null 2>&1 || break
                done
                for i in {1..5}; do
                    certutil -D -d "$db_path" -n "Caddy Intermediate CA - Factory" >/dev/null 2>&1 || break
                done
            }
            
            # Find all Chromium-based browser profile directories
            local chromium_configs=(
                "$HOME/.config/google-chrome"
                "$HOME/.config/chromium"
                "$HOME/.config/BraveSoftware/Brave-Browser"
                "$HOME/.config/microsoft-edge"
            )
            
            for config_dir in "${chromium_configs[@]}"; do
                if [ -d "$config_dir" ]; then
                    for cert_dir in $(find "$config_dir" -type d \( -name "Default" -o -name "Profile *" \) 2>/dev/null); do
                        if [ -f "$cert_dir/Cookies" ] || [ -f "$cert_dir/History" ]; then
                            remove_all_caddy_certs "sql:$cert_dir"
                            if certutil -A -d sql:$cert_dir -t "CT,C,C" -n "Caddy Local CA - Factory" -i "$root_cert_file" >/dev/null 2>&1; then
                                certutil -A -d sql:$cert_dir -t ",," -n "Caddy Intermediate CA - Factory" -i "$intermediate_cert_file" >/dev/null 2>&1 || true
                                browsers_updated=$((browsers_updated + 1))
                            fi
                        fi
                    done
                fi
            done
            
            # Install to system NSS database (used by Chromium browsers as fallback)
            if [ -d ~/.pki/nssdb ]; then
                remove_all_caddy_certs "sql:$HOME/.pki/nssdb"
                if certutil -A -d sql:$HOME/.pki/nssdb -t "CT,C,C" -n "Caddy Local CA - Factory" -i "$root_cert_file" >/dev/null 2>&1; then
                    certutil -A -d sql:$HOME/.pki/nssdb -t ",," -n "Caddy Intermediate CA - Factory" -i "$intermediate_cert_file" >/dev/null 2>&1 || true
                    browsers_updated=$((browsers_updated + 1))
                fi
            fi
            
            # Find Firefox profiles (regular and Snap installations)
            local firefox_dirs=(
                ~/.mozilla/firefox
                ~/snap/firefox/common/.mozilla/firefox
            )
            
            for firefox_base in "${firefox_dirs[@]}"; do
                if [ -d "$firefox_base" ]; then
                    for cert_dir in "$firefox_base"/*.default* "$firefox_base"/*[Pp]rofile*; do
                        if [ -f "$cert_dir/cert9.db" ] || [ -f "$cert_dir/cert8.db" ]; then
                            remove_all_caddy_certs "sql:$cert_dir"
                            if certutil -A -d sql:$cert_dir -t "CT,C,C" -n "Caddy Local CA - Factory" -i "$root_cert_file" >/dev/null 2>&1; then
                                certutil -A -d sql:$cert_dir -t ",," -n "Caddy Intermediate CA - Factory" -i "$intermediate_cert_file" >/dev/null 2>&1 || true
                                browsers_updated=$((browsers_updated + 1))
                            fi
                        fi
                    done
                fi
            done
            
            if [ $browsers_updated -gt 0 ]; then
                log_success "  ✓ Certificates installed to $browsers_updated browser profile(s)"
                log_info "    Supported browsers: Chrome, Chromium, Brave, Edge, Firefox"
                log_info "    Restart browsers to apply changes"
            else
                log_info "  No browser profiles found"
                log_info "    Certificates available in: ${VM_DIR}/"
            fi
        else
            log_warning "  certutil not available - skipping browser certificate installation"
            log_info "    Install libnss3-tools and restart setup if needed"
        fi
        
        if [ "$cert_installed" = "true" ]; then
            log_success "✓ Certificates installed successfully"
            log_info "  Restart your browser to use https://factory.local without warnings"
        else
            log_warning "Certificate installation had issues"
        fi
    else
        log_warning "Could not retrieve Caddy CA certificate from VM"
        log_info "  Certificate will be generated on first HTTPS request"
        log_info "  You may see security warnings until certificates are installed"
    fi
}

# Export functions
export -f install_certificates_on_host
/122            0           0     0     644     2571      `
#!/bin/bash
# install-docker.sh - Install Docker on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_docker_via_ssh() {
    log_info "Installing Docker via SSH..."
    
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
echo "Installing Docker..."
sudo apk add docker docker-compose docker-cli-buildx

echo "Configuring Docker to use cache disk..."
# Configure Docker to store data on the cache disk (vdb)
# This preserves images across reinstalls and keeps system disk small
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json > /dev/null << 'DAEMON_JSON'
{
    "data-root": "/var/cache/factory-build/docker"
}
DAEMON_JSON

# Create the docker directory on cache disk
sudo mkdir -p /var/cache/factory-build/docker

echo "Configuring Docker service..."
sudo rc-update add docker boot
sudo service docker start

# Add foreman to docker group (if not already done)
sudo addgroup foreman docker 2>/dev/null || true

# Wait for Docker to be ready (socket must exist before we can chmod it)
echo "Waiting for Docker socket..."
for i in {1..15}; do
    if [ -S /var/run/docker.sock ] && sudo docker version >/dev/null 2>&1; then
        break
    fi
    sleep 2
done

# Make Docker socket accessible to containers (for Jenkins Docker Cloud agent)
# The Jenkins container runs as uid 1000, which isn't in the host's docker group
# This allows containers to communicate with the Docker daemon
sudo chmod 666 /var/run/docker.sock

# Make this persist across reboots by adding to local startup
sudo tee /etc/local.d/docker-socket.start > /dev/null << 'STARTUP'
#!/bin/sh
# Make Docker socket accessible to containers (Jenkins Docker Cloud agent)
# Wait for socket to exist
for i in 1 2 3 4 5 6 7 8 9 10; do
    if [ -S /var/run/docker.sock ]; then
        chmod 666 /var/run/docker.sock
        break
    fi
    sleep 2
done
STARTUP
sudo chmod +x /etc/local.d/docker-socket.start
sudo rc-update add local default 2>/dev/null || true

echo "Docker version:"
sudo docker version

echo "✓ Docker installed and running"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Docker installed and running"
    else
        log_error "Docker installation failed"
        return 1
    fi
}

# Export functions
export -f install_docker_via_ssh

/141            0           0     0     644     1913      `
#!/bin/bash
# install-jcscripts.sh - Install jcscripts collection on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_jcscripts_via_ssh() {
    log_info "Installing jcscripts collection via SSH..."
    
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << 'EOF'
echo "Installing jcscripts collection..."

# Install jcscripts using official installer
curl -s https://raw.githubusercontent.com/jcgarcia/jcscripts/main/installscripts | bash

# Configure .profile for SSH sessions
cat > ~/.profile << 'PROFILE_EOF'
# ~/.profile: executed by Bourne-compatible login shells

# Set PATH to include user's scripts
if [ -d "$HOME/.scripts" ] ; then
    PATH="$HOME/.scripts:$PATH"
fi

# Source .bashrc for interactive shells
if [ -n "$BASH_VERSION" ]; then
    if [ -f "$HOME/.bashrc" ]; then
        . "$HOME/.bashrc"
    fi
fi

# For Alpine's ash shell
if [ -f "$HOME/.ashrc" ]; then
    ENV="$HOME/.ashrc"
    export ENV
fi
PROFILE_EOF

# Create .ashrc for ash shell (Alpine default)
cat > ~/.ashrc << 'ASHRC_EOF'
# ~/.ashrc: executed by ash for interactive shells

# Set PATH
if [ -d "$HOME/.scripts" ] ; then
    PATH="$HOME/.scripts:$PATH"
fi

# Basic aliases
alias ll='ls -lah'
alias la='ls -A'
ASHRC_EOF

# Fix ownership
chown foreman:foreman ~/.profile ~/.ashrc

echo "✓ jcscripts installed"
echo "  Installed $(ls ~/.scripts/ 2>/dev/null | wc -l) scripts"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ jcscripts installed"
    else
        log_warning "jcscripts installation had issues (continuing anyway)"
    fi
}

# Export functions
export -f install_jcscripts_via_ssh

/163            0           0     0     644     3264      `
#!/bin/bash
# install-k8s-tools.sh - Install Kubernetes tools (kubectl, Helm) on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_kubernetes_tools_via_ssh() {
    log_info "Installing Kubernetes tools via SSH..."
    
    local kubectl_version="$1"
    local helm_version="$2"
    
    # Check if cached on data disk, if not copy from host
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "test -f /var/cache/factory-build/kubectl/kubectl_${kubectl_version}" 2>/dev/null; then
        
        if [ -f "${CACHE_DIR}/kubectl/kubectl_${kubectl_version}" ]; then
            log_info "  Copying kubectl from host cache..."
            scp -i "$VM_SSH_PRIVATE_KEY" -P "$VM_SSH_PORT" \
                -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
                "${CACHE_DIR}/kubectl/kubectl_${kubectl_version}" \
                foreman@localhost:/var/cache/factory-build/kubectl/
        fi
    else
        log_info "  ✓ kubectl already cached on data disk (skipping copy)"
    fi
    
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "test -f /var/cache/factory-build/helm/helm-v${helm_version}-linux-arm64.tar.gz" 2>/dev/null; then
        
        if [ -f "${CACHE_DIR}/helm/helm-v${helm_version}-linux-arm64.tar.gz" ]; then
            log_info "  Copying Helm from host cache..."
            scp -i "$VM_SSH_PRIVATE_KEY" -P "$VM_SSH_PORT" \
                -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
                "${CACHE_DIR}/helm/helm-v${helm_version}-linux-arm64.tar.gz" \
                foreman@localhost:/var/cache/factory-build/helm/
        fi
    else
        log_info "  ✓ Helm already cached on data disk (skipping copy)"
    fi
    
    # Install kubectl and Helm
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << EOF
echo "Installing kubectl..."
if [ -f "/var/cache/factory-build/kubectl/kubectl_${kubectl_version}" ]; then
    sudo cp /var/cache/factory-build/kubectl/kubectl_${kubectl_version} /usr/local/bin/kubectl
    sudo chmod +x /usr/local/bin/kubectl
    kubectl version --client
else
    echo "⚠ kubectl cache not found, skipping"
fi

echo "Installing Helm..."
if [ -f "/var/cache/factory-build/helm/helm-v${helm_version}-linux-arm64.tar.gz" ]; then
    cd /tmp
    tar -zxf /var/cache/factory-build/helm/helm-v${helm_version}-linux-arm64.tar.gz
    sudo mv linux-arm64/helm /usr/local/bin/
    rm -rf linux-arm64
    helm version
else
    echo "⚠ Helm cache not found, skipping"
fi

echo "✓ Kubernetes tools installed"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Kubernetes tools installed"
    else
        log_warning "Kubernetes tools installation had issues (continuing anyway)"
    fi
}

# Export functions
export -f install_kubernetes_tools_via_ssh
/185            0           0     0     644     2132      `
#!/bin/bash
# install-terraform.sh - Install Terraform on Factory VM
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

install_terraform_via_ssh() {
    log_info "Installing Terraform via SSH..."
    
    local terraform_version="$1"
    
    # Check if cached on data disk, if not copy from host
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "test -f /var/cache/factory-build/terraform/terraform_${terraform_version}_linux_arm64.zip" 2>/dev/null; then
        
        if [ -f "${CACHE_DIR}/terraform/terraform_${terraform_version}_linux_arm64.zip" ]; then
            log_info "  Copying Terraform from host cache..."
            scp -i "$VM_SSH_PRIVATE_KEY" -P "$VM_SSH_PORT" \
                -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
                "${CACHE_DIR}/terraform/terraform_${terraform_version}_linux_arm64.zip" \
                foreman@localhost:/var/cache/factory-build/terraform/
        fi
    else
        log_info "  ✓ Terraform already cached on data disk (skipping copy)"
    fi
    
    # Install Terraform
    ssh -i "$VM_SSH_PRIVATE_KEY" -p "$VM_SSH_PORT" \
        -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=30 \
        foreman@localhost << EOF
echo "Installing Terraform..."
if [ -f "/var/cache/factory-build/terraform/terraform_${terraform_version}_linux_arm64.zip" ]; then
    cd /tmp
    unzip -q /var/cache/factory-build/terraform/terraform_${terraform_version}_linux_arm64.zip
    sudo mv terraform /usr/local/bin/
    terraform version
else
    echo "⚠ Terraform cache not found, skipping"
fi

echo "✓ Terraform installed"
EOF
    
    if [ $? -eq 0 ]; then
        log_success "✓ Terraform installed"
    else
        log_warning "Terraform installation had issues (continuing anyway)"
    fi
}

# Export functions
export -f install_terraform_via_ssh
setup-motd.sh/  0           0     0     644     3461      `
#!/bin/bash
# setup-motd.sh - Create welcome banner for Factory VM
#
# Creates /etc/motd with information about installed tools,
# Jenkins access, and quick start guide.

################################################################################
# Setup Welcome Banner (MOTD)
################################################################################

setup_motd_via_ssh() {
    log "Creating welcome banner..."
    
    # Create MOTD via SSH (use bash to avoid ash syntax issues)
    if ssh -i "$VM_SSH_PRIVATE_KEY" \
        -o StrictHostKeyChecking=no \
        -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=10 \
        -p "$VM_SSH_PORT" \
        root@localhost 'bash -s' << 'MOTD_SCRIPT'
cat > /etc/motd << 'MOTD_EOF'
╔═══════════════════════════════════════════════════════════╗
║                                                           ║
║              🏭  Factory VM - ARM64 Build Server          ║
║                                                           ║
╚═══════════════════════════════════════════════════════════╝

Welcome to your automated ARM64 build environment!

📦 Installed Tools:
  • Docker        - Container runtime (docker command)
  • Kubernetes    - kubectl & Helm
  • Terraform     - Infrastructure as Code
  • AWS CLI       - Cloud management
  • Jenkins       - CI/CD automation server
  • Git, Node.js, Python, OpenJDK

🌐 Jenkins CI/CD Server:
  Web UI:    https://factory.local
  Username:  foreman
  Password:  (see ~/.factory-vm/credentials.txt on host)
  
  CLI:       jenkins-factory <command>
             Available on HOST and inside VM
             Examples:
               jenkins-factory who-am-i
               jenkins-factory list-jobs
               jenkins-factory build <job-name>

📁 Storage:
  System:    /         (50 GB)
  Data:      /data     (200 GB) - For build artifacts

🔒 Security:
  • SSH: Key-based authentication only
  • Jenkins: Secure random password
  • HTTPS: Certificate installed on host (trusted connection)

📖 Documentation:
  Factory README: cat /root/FACTORY-README.md
  Installation log: cat /root/factory-install.log

💡 Quick Start:
  1. Configure AWS:  awslogin (on host, then SSH forwards)
  2. Build image:    docker build -t myapp:arm64 .
  3. Access Jenkins: Open https://factory.local in browser

═══════════════════════════════════════════════════════════════
MOTD_EOF
MOTD_SCRIPT
    then
        log_success "  ✓ Welcome banner created"
        return 0
    else
        log_warning "  Failed to create welcome banner (continuing anyway)"
        return 1
    fi
}

################################################################################
# Module Initialization
################################################################################

# Verify this module is being sourced, not executed
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    echo "ERROR: setup-motd.sh should be sourced, not executed directly"
    echo "Usage: source ${BASH_SOURCE[0]}"
    exit 1
fi

# Export functions
export -f setup_motd_via_ssh

vm-bootstrap.sh/0           0     0     644     11041     `
#!/bin/bash
# vm-bootstrap.sh - Bootstrap Factory VM after Alpine installation
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

setup_cache_disk_in_vm() {
    log "Setting up cache disk (vdb) for build cache..."
    
    # Check if cache disk has a filesystem
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "sudo blkid /dev/vdb" 2>/dev/null | grep -q "TYPE=\"ext4\""; then
        
        log_info "Cache disk not formatted - creating ext4 filesystem..."
        ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -p "$VM_SSH_PORT" foreman@localhost "sudo mkfs.ext4 -F -L factory-cache /dev/vdb" >/dev/null 2>&1
        
        log_success "Cache disk formatted (ext4)"
    else
        log_info "Cache disk already formatted (reusing preserved cache)"
    fi
    
    # Create mount point and mount cache disk
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "sudo mkdir -p /var/cache/factory-build && sudo mount /dev/vdb /var/cache/factory-build && sudo chown -R foreman:foreman /var/cache/factory-build"
    
    # Add to fstab for auto-mount on boot
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "grep -q '/dev/vdb' /etc/fstab || echo '/dev/vdb /var/cache/factory-build ext4 defaults 0 2' | sudo tee -a /etc/fstab" >/dev/null
    
    log_success "Cache disk mounted at /var/cache/factory-build"
}

setup_data_disk_in_vm() {
    log "Setting up data disk (vdc) for Jenkins workspaces..."
    
    # Check if data disk has a filesystem
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "sudo blkid /dev/vdc" 2>/dev/null | grep -q "TYPE=\"ext4\""; then
        
        log_info "Data disk not formatted - creating ext4 filesystem..."
        ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -p "$VM_SSH_PORT" foreman@localhost "sudo mkfs.ext4 -F -L factory-data /dev/vdc" >/dev/null 2>&1
        
        log_success "Data disk formatted (ext4)"
    else
        log_info "Data disk already formatted"
    fi
    
    # Create mount point and mount data disk at /var/lib/jenkins
    # Set ownership to UID 1000:1000 (jenkins user in container)
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "sudo mkdir -p /var/lib/jenkins && sudo mount /dev/vdc /var/lib/jenkins && sudo chown -R 1000:1000 /var/lib/jenkins"
    
    # Add to fstab for auto-mount on boot
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "grep -q '/dev/vdc' /etc/fstab || echo '/dev/vdc /var/lib/jenkins ext4 defaults 0 2' | sudo tee -a /etc/fstab" >/dev/null
    
    log_success "Data disk mounted at /var/lib/jenkins"
}

configure_installed_vm() {
    log "Configuring installed Factory VM..."
    
    # Start VM using the start script
    log_info "Starting VM from installed system..."
    if ! "${VM_DIR}/start-factory.sh" >/dev/null 2>&1; then
        log_error "Failed to start Factory VM"
        exit 1
    fi
    
    sleep 5
    
    # Remove old SSH host key from known_hosts (VM was reinstalled)
    log_info "Removing old SSH host key from known_hosts..."
    ssh-keygen -f "$HOME/.ssh/known_hosts" -R "[localhost]:${VM_SSH_PORT}" 2>/dev/null || true
    
    # Wait for SSH port to open
    log_info "Waiting for SSH port to open..."
    local count=0
    while ! nc -z localhost "$VM_SSH_PORT" 2>/dev/null && [ $count -lt 60 ]; do
        sleep 2
        ((count++))
        echo -n "."
    done
    echo ""
    
    if [ $count -ge 60 ]; then
        log_error "VM failed to start - SSH port never opened"
        exit 1
    fi
    
    # Port is open, but SSH may not be fully ready - Alpine boot is SLOW under TCG emulation
    log_info "Port open, waiting for Alpine to finish booting (this is slow under TCG emulation)..."
    log_info "This can take 3-5 minutes on TCG emulation, please be patient..."
    local ssh_test_attempts=0
    local max_attempts=60  # 60 attempts × 5 seconds = 300 seconds (5 minutes)
    while [ $ssh_test_attempts -lt $max_attempts ]; do
        if ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -o ConnectTimeout=10 -o ServerAliveInterval=5 -o ServerAliveCountMax=2 \
            -p "$VM_SSH_PORT" root@localhost "echo ready" >/dev/null 2>&1; then
            log_info "SSH is ready!"
            break
        fi
        ssh_test_attempts=$((ssh_test_attempts + 1))
        if [ $((ssh_test_attempts % 6)) -eq 0 ]; then
            log_info "Still waiting... ($((ssh_test_attempts * 5)) seconds elapsed)"
        fi
        sleep 5
    done
    
    if [ $ssh_test_attempts -ge $max_attempts ]; then
        log_error "SSH did not become ready after 300 seconds"
        log_error "This might indicate a problem with the Alpine installation"
        exit 1
    fi
    
    # Create foreman user
    log "Creating foreman user with sudo privileges..."
    if ! ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -o ConnectTimeout=60 -o ServerAliveInterval=5 -p "$VM_SSH_PORT" root@localhost << EOF
# Create foreman user
adduser -D foreman
echo "foreman:${FOREMAN_OS_PASSWORD}" | chpasswd

# Add to necessary groups
addgroup foreman wheel
addgroup foreman docker

# Configure doas (Alpine's sudo alternative)
apk add doas
echo "permit nopass :wheel" > /etc/doas.d/doas.conf
chmod 600 /etc/doas.d/doas.conf

# Configure sudoers for passwordless sudo (Alpine installs real sudo as dependency)
# Create sudoers.d directory if it doesn't exist
mkdir -p /etc/sudoers.d
# Allow wheel group to use sudo without password
echo "%wheel ALL=(ALL:ALL) NOPASSWD: ALL" > /etc/sudoers.d/wheel
chmod 440 /etc/sudoers.d/wheel

# Setup SSH directory
mkdir -p /home/foreman/.ssh
chmod 700 /home/foreman/.ssh

# Copy SSH key from root to foreman
cp /root/.ssh/authorized_keys /home/foreman/.ssh/authorized_keys
chmod 600 /home/foreman/.ssh/authorized_keys
chown -R foreman:foreman /home/foreman/.ssh

# Set bash as default shell
apk add bash
sed -i 's|/home/foreman:/bin/ash|/home/foreman:/bin/bash|' /etc/passwd

# Install utilities needed for data disk setup and SSH commands
# - e2fsprogs-extra: provides mkfs.ext4 (for formatting if needed)
# - blkid: provides blkid command (for detecting existing filesystem)
apk add e2fsprogs-extra blkid

# Create sudo wrapper script for compatibility (Alpine uses doas)
# This works in SSH non-login shell contexts
cat > /usr/local/bin/sudo << 'SUDO_WRAPPER'
#!/bin/sh
exec doas "\$@"
SUDO_WRAPPER
chmod 755 /usr/local/bin/sudo

# Create .bashrc for foreman user (required for jcscripts)
cat > /home/foreman/.bashrc << 'BASHRC_INIT'
# ~/.bashrc: executed by bash for non-login shells

# If not running interactively, don't do anything
case $- in
    *i*) ;;
      *) return;;
esac

# Basic environment
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
export EDITOR=vim

# Aliases
alias ll='ls -lah'
alias la='ls -A'
alias l='ls -CF'

# Command history
HISTCONTROL=ignoredups:ignorespace
HISTSIZE=1000
HISTFILESIZE=2000

# Enable bash completion
if [ -f /etc/bash/bashrc.d/bash_completion.sh ]; then
    . /etc/bash/bashrc.d/bash_completion.sh
fi
BASHRC_INIT

chown foreman:foreman /home/foreman/.bashrc

# Harden SSH configuration - disable password authentication
cat >> /etc/ssh/sshd_config << 'SSH_CONFIG'

# Factory VM Security Configuration
# Disable password authentication - SSH keys only
PasswordAuthentication no
PermitRootLogin prohibit-password
PubkeyAuthentication yes
ChallengeResponseAuthentication no
SSH_CONFIG

# Restart SSH to apply changes
rc-service sshd restart

# Wait for SSH to fully restart before allowing external connections
sleep 5

echo "✓ Foreman user created"
echo "✓ SSH hardened (keys only, no password authentication)"
EOF
    then
        log_error "Failed to create foreman user"
        exit 1
    fi
    
    # Add SSH public key
    log "Adding SSH public key for foreman..."
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" root@localhost << EOF
echo '$(cat "$VM_SSH_PUBLIC_KEY")' > /home/foreman/.ssh/authorized_keys
chmod 600 /home/foreman/.ssh/authorized_keys
chown -R foreman:foreman /home/foreman/.ssh
EOF

    # Wait for SSH to be fully ready after sshd restart
    log_info "Waiting for SSH to be fully ready..."
    local ssh_ready=0
    for i in {1..30}; do
        if ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            -o ConnectTimeout=5 -p "$VM_SSH_PORT" foreman@localhost "echo OK" >/dev/null 2>&1; then
            ssh_ready=1
            break
        fi
        sleep 2
    done
    
    if [ $ssh_ready -eq 0 ]; then
        log_error "SSH did not become ready for foreman user in time"
        exit 1
    fi
    
    log_success "SSH is ready"
    
    # Give SSH/SCP a bit more time to fully stabilize after restart
    sleep 5

    # Setup cache disk for persistent build cache (vdb)
    setup_cache_disk_in_vm
    
    # Setup data disk for Jenkins workspaces (vdc)
    setup_data_disk_in_vm
    
    # Prepare cache directories
    log_info "Preparing cache directories..."
    ssh -i "$VM_SSH_PRIVATE_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
        -p "$VM_SSH_PORT" foreman@localhost "mkdir -p /var/cache/factory-build/terraform /var/cache/factory-build/kubectl /var/cache/factory-build/helm /var/cache/factory-build/awscli /var/cache/factory-build/ansible /var/cache/factory-build/jenkins"
    
    # Run Phase 3 SSH-based installations
    log ""
    log "Installing tools via SSH..."
    
    # Get tool versions
    local terraform_version=$(get_latest_terraform_version)
    local kubectl_version=$(get_latest_kubectl_version)
    local helm_version=$(get_latest_helm_version)
    
    # Install in order
    install_base_packages_via_ssh
    install_docker_via_ssh
    install_caddy_via_ssh
    install_kubernetes_tools_via_ssh "$kubectl_version" "$helm_version"
    install_terraform_via_ssh "$terraform_version"
    install_aws_cli_via_ssh
    install_jcscripts_via_ssh
    install_jenkins_via_ssh
    configure_jenkins_via_ssh
    
    # Setup MOTD
    setup_motd_via_ssh
    
    log_success "All tools installed successfully"
}

# Export functions
export -f setup_cache_disk_in_vm setup_data_disk_in_vm configure_installed_vm

vm-lifecycle.sh/0           0     0     644     11209     `
#!/bin/bash
# vm-lifecycle.sh - VM creation and lifecycle management
# Part of Phase 3.5 modular architecture

# Prevent direct execution
if [ "${BASH_SOURCE[0]}" -ef "$0" ]; then
    echo "Error: This script should be sourced, not executed directly"
    exit 1
fi

ensure_qemu() {
    log "Checking QEMU installation..."
    
    if command -v qemu-system-aarch64 &> /dev/null; then
        log "  ✓ QEMU already installed"
        return 0
    fi
    
    log "Installing QEMU..."
    
    if [ -f "${SCRIPT_DIR}/install-qemu.sh" ]; then
        "${SCRIPT_DIR}/install-qemu.sh"
    else
        log_error "QEMU not found. Please install manually:"
        log_info "  Ubuntu: sudo apt-get install qemu-system-arm qemu-efi-aarch64"
        log_info "  macOS: brew install qemu"
        exit 1
    fi
}

check_dependencies() {
    log "Checking dependencies..."
    
    local missing=()
    
    for cmd in curl sshpass ssh-keygen expect nc; do
        if ! command -v $cmd &> /dev/null; then
            missing+=($cmd)
        fi
    done
    
    if [ ${#missing[@]} -gt 0 ]; then
        log_info "Missing dependencies: ${missing[*]}"
        log_info "Installing missing packages..."
        
        # Detect package manager and install
        if command -v apt-get &> /dev/null; then
            sudo apt-get update -qq
            sudo apt-get install -y -qq ${missing[*]}
        elif command -v dnf &> /dev/null; then
            sudo dnf install -y -q ${missing[*]}
        elif command -v yum &> /dev/null; then
            sudo yum install -y -q ${missing[*]}
        elif command -v brew &> /dev/null; then
            brew install ${missing[*]}
        else
            log_error "Unable to auto-install dependencies. Package manager not found."
            log_info "Please install manually: ${missing[*]}"
            exit 1
        fi
        
        # Verify installation
        local still_missing=()
        for cmd in ${missing[@]}; do
            if ! command -v $cmd &> /dev/null; then
                still_missing+=($cmd)
            fi
        done
        
        if [ ${#still_missing[@]} -gt 0 ]; then
            log_error "Failed to install: ${still_missing[*]}"
            log_info "Please install manually and try again"
            exit 1
        fi
        
        log "  ✓ All dependencies installed successfully"
    else
        log "  ✓ All dependencies satisfied"
    fi
}

setup_ssh_keys() {
    log "Setting up SSH keys for foreman user..."
    
    local ssh_dir="${HOME}/.ssh"
    local private_key="${ssh_dir}/${SSH_KEY_NAME}"
    local public_key="${ssh_dir}/${SSH_KEY_NAME}.pub"
    
    mkdir -p "$ssh_dir"
    chmod 700 "$ssh_dir"
    
    if [ -f "$private_key" ]; then
        log "  ✓ SSH key already exists: $private_key"
    else
        log "  Generating SSH key pair..."
        ssh-keygen -t ed25519 -f "$private_key" -N "" -C "foreman@factory"
        log "  ✓ SSH key generated"
    fi
    
    # Export for later use
    export VM_SSH_PRIVATE_KEY="$private_key"
    export VM_SSH_PUBLIC_KEY="$public_key"
}

download_alpine() {
    log "Downloading Alpine Linux ISO..."
    
    # Auto-detect latest Alpine version if not already set
    if [ -z "$ALPINE_VERSION" ]; then
        ALPINE_VERSION=$(get_latest_alpine_version)
        log_info "  Auto-detected Alpine version: ${ALPINE_VERSION}"
    fi
    
    # Get the latest patch version from Alpine's latest-releases.yaml
    local full_version=$(curl -s https://dl-cdn.alpinelinux.org/alpine/v${ALPINE_VERSION}/releases/${ALPINE_ARCH}/latest-releases.yaml 2>/dev/null | grep -m1 'version:' | awk '{print $2}')
    
    # Fallback to .1 if detection fails
    if [ -z "$full_version" ]; then
        full_version="${ALPINE_VERSION}.1"
        log_info "  Using fallback version: ${full_version}"
    else
        log_info "  Latest release: ${full_version}"
    fi
    
    # Set ISO name and URL
    ALPINE_ISO="alpine-virt-${full_version}-${ALPINE_ARCH}.iso"
    ALPINE_ISO_URL="https://dl-cdn.alpinelinux.org/alpine/v${ALPINE_VERSION}/releases/${ALPINE_ARCH}/${ALPINE_ISO}"
    
    # Check cache first (in repository directory)
    local cached_iso="${CACHE_DIR}/alpine/${ALPINE_ISO}"
    if [ -f "$cached_iso" ]; then
        log_info "  Using cached Alpine ISO: ${ALPINE_ISO}"
        mkdir -p "${VM_DIR}/isos"
        cp "$cached_iso" "${VM_DIR}/isos/${ALPINE_ISO}"
        log "  ✓ ISO copied from cache"
        return 0
    fi
    
    # Download to cache, then copy to VM directory
    mkdir -p "${CACHE_DIR}/alpine"
    mkdir -p "${VM_DIR}/isos"
    
    log_info "  Downloading from: ${ALPINE_ISO_URL}"
    log_info "  Caching to: ${cached_iso}"
    curl -L --progress-bar -o "$cached_iso" "${ALPINE_ISO_URL}"
    cp "$cached_iso" "${VM_DIR}/isos/${ALPINE_ISO}"
    log "  ✓ ISO downloaded and cached"
}

create_disks() {
    log "Creating VM disks..."
    
    if [ ! -f "$SYSTEM_DISK" ]; then
        qemu-img create -f qcow2 "$SYSTEM_DISK" "$SYSTEM_DISK_SIZE"
        log "  ✓ System disk created (${SYSTEM_DISK_SIZE})"
    else
        log "  ✓ System disk exists"
    fi
    
    # Check for preserved cache disk from previous test
    local cache_disk_backup="${HOME}/.factory-vm/cache-backup.qcow2"
    
    if [ ! -f "$CACHE_DISK" ]; then
        # Check if we have a preserved cache disk to restore
        if [ -f "$cache_disk_backup" ]; then
            log_info "Restoring preserved cache disk from previous installation..."
            cp "$cache_disk_backup" "$CACHE_DISK"
            rm -f "$cache_disk_backup"
            local size=$(du -h "$CACHE_DISK" | cut -f1)
            log_success "Cache disk restored (${size}) - cache will be reused!"
        else
            qemu-img create -f qcow2 "$CACHE_DISK" "$CACHE_DISK_SIZE"
            log "  ✓ Cache disk created (${CACHE_DISK_SIZE})"
        fi
    else
        log "  ✓ Cache disk exists"
    fi
    
    # Create data disk for Jenkins workspaces
    if [ ! -f "$DATA_DISK" ]; then
        qemu-img create -f qcow2 "$DATA_DISK" "$DATA_DISK_SIZE"
        log "  ✓ Data disk created (${DATA_DISK_SIZE})"
    else
        log "  ✓ Data disk exists"
    fi
}

find_uefi_firmware() {
    # UEFI firmware requires both CODE (read-only) and VARS (read-write) files
    # We'll return the CODE file path and create a writable VARS copy
    local firmware_paths=(
        "/usr/share/AAVMF/AAVMF_CODE.fd"
        "/usr/share/qemu-efi-aarch64/QEMU_EFI.fd"
        "/usr/share/edk2/aarch64/QEMU_EFI.fd"
        "/opt/homebrew/share/qemu/edk2-aarch64-code.fd"
        "/usr/share/qemu/edk2-aarch64-code.fd"
    )
    
    for path in "${firmware_paths[@]}"; do
        if [ -f "$path" ]; then
            echo "$path"
            return 0
        fi
    done
    
    log_error "UEFI firmware not found"
    log_info "Install: sudo apt-get install qemu-efi-aarch64"
    exit 1
}

find_uefi_vars() {
    # Find the corresponding VARS file for the firmware
    local vars_paths=(
        "/usr/share/AAVMF/AAVMF_VARS.fd"
        "/usr/share/qemu-efi-aarch64/QEMU_VARS.fd"
        "/usr/share/edk2/aarch64/QEMU_VARS.fd"
        "/opt/homebrew/share/qemu/edk2-arm-vars.fd"
        "/usr/share/qemu/edk2-arm-vars.fd"
    )
    
    for path in "${vars_paths[@]}"; do
        if [ -f "$path" ]; then
            echo "$path"
            return 0
        fi
    done
    
    # VARS file not critical if not found
    echo ""
    return 0
}

start_vm_for_install() {
    log "Starting VM for Alpine installation..."
    
    local uefi_fw="/usr/share/qemu-efi-aarch64/QEMU_EFI.fd"
    local iso_path="${VM_DIR}/isos/${ALPINE_ISO}"
    
    # Check UEFI firmware exists
    if [ ! -f "$uefi_fw" ]; then
        log_error "UEFI firmware not found at $uefi_fw"
        log_info "Install with: sudo apt-get install qemu-efi-aarch64"
        exit 1
    fi
    
    # Determine QEMU acceleration
    # KVM only works when host and guest architectures match
    local host_arch=$(uname -m)
    local qemu_accel=""
    
    if [ "$host_arch" = "aarch64" ] && [ -e /dev/kvm ] && [ -r /dev/kvm ] && [ -w /dev/kvm ]; then
        # ARM64 host with KVM support - can use KVM for ARM64 guest
        qemu_accel="-accel kvm"
        log_info "  Using KVM acceleration (ARM64 native)"
    elif [ "$host_arch" = "x86_64" ]; then
        # x86_64 host cannot use KVM for ARM64 guest - use TCG (software emulation)
        qemu_accel="-accel tcg"
        log_info "  Using TCG emulation (x86_64 host → ARM64 guest)"
        log_info "  Note: Builds will be slower than native ARM64"
    else
        # Fallback to TCG for any other scenario
        qemu_accel="-accel tcg"
        log_info "  Using TCG emulation"
    fi
    
    log ""
    log "═══════════════════════════════════════════════════════════"
    log "  Starting automated Alpine installation"
    log "═══════════════════════════════════════════════════════════"
    log ""
    
    if [ "$qemu_accel" = "-accel tcg" ]; then
        log "  Installing Alpine Linux to disk (TCG emulation - SLOW)..."
        log "  ⚠️  WARNING: TCG emulation is 10-20x slower than KVM"
        log "  This will take 15-20 minutes (automated)"
        log "  Alpine boot alone can take 10-15 minutes on x86_64 hosts"
        log ""
        log "  Please be patient - the installation is working, just very slow"
    else
        log "  Installing Alpine Linux to disk (KVM acceleration)..."
        log "  This will take 3-5 minutes (automated)"
    fi
    log ""
    
    # Build complete QEMU command using -bios (same as start-factory.sh)
    # NOTE: Data disk NOT attached during Alpine installation to prevent setup-disk from erasing it
    # The data disk will be attached when VM starts via start-factory.sh
    local qemu_cmd="qemu-system-aarch64 -M virt $qemu_accel -cpu cortex-a72 -smp $VM_CPUS -m $VM_MEMORY -bios $uefi_fw -drive file=$SYSTEM_DISK,if=virtio,format=qcow2 -cdrom $iso_path -device virtio-net-pci,netdev=net0 -netdev user,id=net0,hostfwd=tcp::${VM_SSH_PORT}-:22 -nographic"
    
    # Export variables for expect script
    export VM_HOSTNAME VM_ROOT_PASSWORD
    export VM_SSH_PUBLIC_KEY_CONTENT="$(cat "$VM_SSH_PUBLIC_KEY")"
    export QEMU_COMMAND="$qemu_cmd"
    
    # Run automated installation using external expect script (in parent tools/ directory)
    if ! expect "$(dirname "$SCRIPT_DIR")/alpine-install.exp"; then
        # Reset terminal in case expect left it in a bad state
        reset 2>/dev/null || stty sane 2>/dev/null || true
        log_error "Alpine installation failed"
        exit 1
    fi
    
    # Reset terminal to clean state after expect/QEMU
    reset 2>/dev/null || stty sane 2>/dev/null || true
    
    log ""
    log "✓ Alpine installation complete"
    log "  VM has been powered off"
}

# Export functions
export -f ensure_qemu check_dependencies setup_ssh_keys download_alpine
export -f create_disks find_uefi_firmware find_uefi_vars start_vm_for_install

/207            0           0     0     644     6662      `
#!/usr/bin/env bash
################################################################################
# Factory VM Data Disk Expansion Script
# 
# Expands the data disk for Factory VM when you need more space for builds,
# Docker images, or Jenkins artifacts.
#
# Usage:
#   ./expand-data-disk.sh [size_in_GB]
#
# Example:
#   ./expand-data-disk.sh 100   # Expand to 100GB
#   ./expand-data-disk.sh 200   # Expand to 200GB
#
# Prerequisites:
#   - Factory VM must be stopped
#   - You must have sufficient disk space on host
################################################################################

set -euo pipefail

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Logging functions
log() { echo -e "${GREEN}[$(date +'%H:%M:%S')]${NC} $*"; }
log_info() { echo -e "${BLUE}[INFO]${NC}   $*"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $*"; }
log_error() { echo -e "${RED}[ERROR]${NC}  $*" >&2; }
log_success() { echo -e "${GREEN}[✓]${NC} $*"; }

# Default paths
VM_DIR="${HOME}/vms/factory"
DATA_DISK="${VM_DIR}/factory-data.qcow2"

################################################################################
# Print usage
################################################################################

usage() {
    cat << EOF
${GREEN}Factory VM Data Disk Expansion${NC}

Usage:
  $0 <size_in_GB>

Examples:
  $0 100    # Expand data disk to 100GB
  $0 200    # Expand data disk to 200GB
  $0 500    # Expand data disk to 500GB

Current disk status:
EOF
    
    if [ -f "$DATA_DISK" ]; then
        local current_size=$(qemu-img info "$DATA_DISK" | grep "virtual size" | awk '{print $3}')
        local current_usage=$(du -h "$DATA_DISK" | cut -f1)
        echo "  Disk file: $DATA_DISK"
        echo "  Virtual size: ${current_size}"
        echo "  Actual usage: ${current_usage} (sparse file)"
    else
        echo "  ${YELLOW}Data disk not found: $DATA_DISK${NC}"
        echo "  Run setup-factory-vm.sh first to create the VM"
    fi
    
    exit 1
}

################################################################################
# Check if VM is running
################################################################################

check_vm_not_running() {
    if pgrep -f "qemu-system-aarch64.*factory.qcow2" > /dev/null; then
        log_error "Factory VM is currently running!"
        log_info "Stop the VM first: ~/vms/factory/stop-factory.sh"
        exit 1
    fi
}

################################################################################
# Expand disk
################################################################################

expand_disk() {
    local new_size="$1"
    
    log "Expanding Factory VM data disk to ${new_size}GB..."
    
    # Check if disk exists
    if [ ! -f "$DATA_DISK" ]; then
        log_error "Data disk not found: $DATA_DISK"
        log_info "Run setup-factory-vm.sh first to create the VM"
        exit 1
    fi
    
    # Check available host disk space
    local available_gb=$(df -BG "$VM_DIR" | tail -1 | awk '{print $4}' | sed 's/G//')
    if [ "$available_gb" -lt "$new_size" ]; then
        log_error "Insufficient disk space on host"
        log_info "Required: ${new_size}GB, Available: ${available_gb}GB"
        log_info "Note: QCOW2 files are sparse - they grow as needed"
        log_warning "Continuing anyway (sparse files only use actual data space)..."
    fi
    
    # Show current size
    local current_size=$(qemu-img info "$DATA_DISK" | grep "virtual size" | awk '{print $3}')
    log_info "Current virtual size: ${current_size}"
    log_info "Target size: ${new_size}GB"
    
    # Expand the disk image
    log_info "Expanding QCOW2 image (this is instant)..."
    if qemu-img resize "$DATA_DISK" "${new_size}G"; then
        log_success "Disk image expanded successfully"
    else
        log_error "Failed to expand disk image"
        exit 1
    fi
    
    # Show new size
    local new_virtual=$(qemu-img info "$DATA_DISK" | grep "virtual size" | awk '{print $3}')
    log_success "New virtual size: ${new_virtual}"
    
    log ""
    log_info "Next steps:"
    log_info "1. Start Factory VM: ~/vms/factory/start-factory.sh"
    log_info "2. Connect: ssh factory"
    log_info "3. Expand the filesystem inside VM:"
    log_info ""
    log_info "   ${BLUE}# Check current partition${NC}"
    log_info "   sudo fdisk -l /dev/vdb"
    log_info ""
    log_info "   ${BLUE}# Grow partition (if using parted)${NC}"
    log_info "   sudo parted /dev/vdb resizepart 1 100%"
    log_info ""
    log_info "   ${BLUE}# Grow filesystem (ext4)${NC}"
    log_info "   sudo resize2fs /dev/vdb1"
    log_info ""
    log_info "   ${BLUE}# OR grow filesystem (xfs)${NC}"
    log_info "   sudo xfs_growfs /data"
    log_info ""
    log_info "   ${BLUE}# Verify new size${NC}"
    log_info "   df -h /data"
    log ""
    log_success "Data disk expansion complete!"
}

################################################################################
# Main
################################################################################

main() {
    # Show header
    echo ""
    echo "╔═══════════════════════════════════════════════════════════╗"
    echo "║                                                           ║"
    echo "║        Factory VM Data Disk Expansion                    ║"
    echo "║                                                           ║"
    echo "╚═══════════════════════════════════════════════════════════╝"
    echo ""
    
    # Check arguments
    if [ $# -eq 0 ]; then
        usage
    fi
    
    local new_size="$1"
    
    # Validate size is a number
    if ! [[ "$new_size" =~ ^[0-9]+$ ]]; then
        log_error "Size must be a number in GB"
        usage
    fi
    
    # Validate size is reasonable (at least 50GB, max 2TB)
    if [ "$new_size" -lt 50 ]; then
        log_error "Size must be at least 50GB"
        exit 1
    fi
    
    if [ "$new_size" -gt 2000 ]; then
        log_error "Size must be less than 2TB (2000GB)"
        exit 1
    fi
    
    # Check if qemu-img is available
    if ! command -v qemu-img >/dev/null 2>&1; then
        log_error "qemu-img not found"
        log_info "Install: sudo apt install qemu-utils"
        exit 1
    fi
    
    # Check VM is not running
    check_vm_not_running
    
    # Expand the disk
    expand_disk "$new_size"
}

main "$@"
/228            0           0     0     644     2372      `
#!/bin/bash
################################################################################
# Install Android SDK on Factory VM
#
# This script installs Android SDK, build tools, and Gradle for mobile builds
# Run this on your host machine, it will SSH into the VM to install.
#
# Usage:
#   ./install-android-sdk.sh
#
################################################################################

set -e

echo "Installing Android SDK on Factory VM..."
echo ""

# Download and install Android SDK
ssh factory << 'REMOTE_INSTALL'
set -e

ANDROID_SDK_VERSION="11076708"
ANDROID_SDK_URL="https://dl.google.com/android/repository/commandlinetools-linux-${ANDROID_SDK_VERSION}_latest.zip"
ANDROID_HOME="/opt/android-sdk"

echo "Downloading Android SDK Command Line Tools..."
sudo mkdir -p ${ANDROID_HOME}/cmdline-tools
cd /tmp
wget -q ${ANDROID_SDK_URL} -O android-sdk.zip

echo "Extracting Android SDK..."
sudo unzip -q android-sdk.zip -d ${ANDROID_HOME}/cmdline-tools
sudo mv ${ANDROID_HOME}/cmdline-tools/cmdline-tools ${ANDROID_HOME}/cmdline-tools/latest
rm android-sdk.zip

# Set environment variables
echo "Configuring environment..."
sudo tee -a /etc/profile.d/android-sdk.sh > /dev/null << 'PROFILE'
export ANDROID_HOME=/opt/android-sdk
export PATH=$PATH:$ANDROID_HOME/cmdline-tools/latest/bin
export PATH=$PATH:$ANDROID_HOME/platform-tools
export PATH=$PATH:$ANDROID_HOME/build-tools/34.0.0
PROFILE

source /etc/profile.d/android-sdk.sh

# Accept licenses and install essentials
echo "Installing Android SDK packages..."
yes | ${ANDROID_HOME}/cmdline-tools/latest/bin/sdkmanager --licenses || true
${ANDROID_HOME}/cmdline-tools/latest/bin/sdkmanager "platform-tools" "platforms;android-34" "build-tools;34.0.0"

# Install Gradle
echo "Installing Gradle..."
GRADLE_VERSION="8.5"
cd /tmp
wget -q https://services.gradle.org/distributions/gradle-${GRADLE_VERSION}-bin.zip
sudo unzip -q gradle-${GRADLE_VERSION}-bin.zip -d /opt
sudo ln -sf /opt/gradle-${GRADLE_VERSION}/bin/gradle /usr/local/bin/gradle
rm gradle-${GRADLE_VERSION}-bin.zip

echo "✓ Android SDK installation complete"
REMOTE_INSTALL

echo ""
echo "✓ Android SDK and Gradle installed successfully!"
echo ""
echo "Verify installation:"
echo "  ssh factory 'echo \$ANDROID_HOME'"
echo "  ssh factory 'gradle --version'"
echo ""
echo "Environment variables set in /etc/profile.d/android-sdk.sh"
echo ""
/252            0           0     0     644     926       `
#!/bin/bash
################################################################################
# Install Ansible on Factory VM
#
# This script installs Ansible and its AWS dependencies (boto3, botocore)
# Run this on your host machine, it will SSH into the VM to install.
#
# Usage:
#   ./install-ansible.sh
#
################################################################################

set -e

echo "Installing Ansible on Factory VM..."
echo ""

if ! ssh factory 'command -v pip3 >/dev/null 2>&1'; then
    echo "Error: Python3 pip not found in VM"
    exit 1
fi

echo "Installing Ansible and AWS dependencies..."
ssh factory 'sudo apk add --no-cache py3-pip python3-dev build-base libffi-dev openssl-dev'
ssh factory 'sudo pip3 install --break-system-packages ansible boto3 botocore'

echo ""
echo "✓ Ansible installed successfully!"
echo ""
echo "Verify installation:"
echo "  ssh factory 'ansible --version'"
echo ""
